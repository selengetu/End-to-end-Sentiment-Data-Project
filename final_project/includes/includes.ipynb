{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9f7c42-13e2-4591-b7c3-415e7369bb68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip mlflow transformers==4.35.2 emoji==0.6.0 --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72b0667-b55f-4f04-9712-3631c469e67d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# restart the kernel after loading dependencies\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f254a0f4-bae7-441e-bbb1-8814f4c161c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Set the notebooks starting time.\n",
    "START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9af3e6-3425-46ae-8d0d-3f80efc55a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#/Volumes/voc_catalog/default/voc_volume\n",
    "# Specify the bucket name, url, and path\n",
    "TWEET_BUCKET_NAME = 'voc-75-databricks-data'\n",
    "TWEET_BUCKET_URL = f\"https://{TWEET_BUCKET_NAME}.s3.amazonaws.com/\"\n",
    "TWEET_SOURCE_PATH = f\"s3a://{TWEET_BUCKET_NAME}/voc_volume/\"\n",
    "\n",
    "# setup storage for this user\n",
    "#username = spark.sql(\"SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')\").first()[0]\n",
    "\n",
    "USER_NAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get().split('@')[0]\n",
    "USER_DIR = f'/tmp/{USER_NAME}/'\n",
    "\n",
    "BRONZE_CHECKPOINT = USER_DIR + 'bronze.checkpoint'\n",
    "BRONZE_DELTA = USER_DIR + 'bronze.delta'\n",
    "\n",
    "SILVER_CHECKPOINT = USER_DIR + 'silver.checkpoint'\n",
    "SILVER_DELTA = USER_DIR + 'silver.delta'\n",
    "\n",
    "GOLD_CHECKPOINT = USER_DIR + 'gold.checkpoint'\n",
    "GOLD_DELTA = USER_DIR + 'gold.delta'\n",
    "\n",
    "MODEL_NAME = \"HF_TWEET_SENTIMENT\" #USER_NAME + \"_Model\"\n",
    "\n",
    "# https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "HF_MODEL_NAME = \"finiteautomata/bertweet-base-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "676336a8-f326-4dcb-85e8-dc8f3a35e6e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7dcabd-f1b2-4056-a81c-837f545f3b7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "displayHTML(f\"\"\"\n",
    "<H2>VERY IMPORTANT TO UNDERSTAND THE USE OF THESE VARIABLES!<br> Please ask if you are confused about their use.</H2>\n",
    "<table border=1>\n",
    "<tr><td><b>Variable Name</b></td><td><b>Value</b></td><td><b>Description</b></td></tr>\n",
    "<tr><td>TWEET_BUCKET_NAME</td><td>{TWEET_BUCKET_NAME}</td><td>AWS S3 Bucket Name where the tweets are coming into your system.</td></tr>\n",
    "<tr><td>TWEET_BUCKET_URL</td><td>{TWEET_BUCKET_URL}</td><td>AWS S3 Bucket URL where the tweets are coming into your system.</td></tr>\n",
    "<tr><td>TWEET_SOURCE_PATH</td><td>{TWEET_SOURCE_PATH}</td><td>AWS S3 Path where the tweets are coming into your system.</td></tr>\n",
    "<tr><td>USER_DIR</td><td>{USER_DIR}</td><td>Path to the local storage (dbfs) for your project.</td></tr>\n",
    "<tr><td>BRONZE_CHECKPOINT</td><td>{BRONZE_CHECKPOINT}</td><td>Store your Bronze Checkpoint data here.</td></tr>\n",
    "<tr><td>BRONZE_DELTA</td><td>{BRONZE_DELTA}</td><td>Store your Bronze Delta Table here.</td></tr>\n",
    "<tr><td>SILVER_CHECKPOINT</td><td>{SILVER_CHECKPOINT}</td><td>Store your Silver Checkpoint data here.</td></tr>\n",
    "<tr><td>SILVER_DELTA</td><td>{SILVER_DELTA}</td><td>Store your Silver Delta Table here.</td></tr>\n",
    "<tr><td>GOLD_CHECKPOINT</td><td>{GOLD_CHECKPOINT}</td><td>Store your Gold Checkpoint data here.</td></tr>\n",
    "<tr><td>GOLD_DELTA</td><td>{GOLD_DELTA}</td><td>Store your Gold Delta Table here.</td></tr>\n",
    "<tr><td>MODEL_NAME</td><td>{MODEL_NAME}</td><td>Load this production model</td></tr>\n",
    "<tr><td>HF_MODEL_NAME</td><td>{HF_MODEL_NAME}</td><td>The Hugging Face Model for Tweet sentiment classification: https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis </td></tr>\n",
    "</table>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c575d0-a22e-4c8e-84bc-141e3100ea2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('the includes are included')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e896426b-642a-4c25-90c0-71cbdfe9d6d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_extract,\n",
    "    to_timestamp,\n",
    "    expr,\n",
    "    regexp_replace,\n",
    "    pandas_udf,\n",
    ")\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining **Common** functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_stream_writer(\n",
    "    df: DataFrame,\n",
    "    checkpoint: str,\n",
    "    queryName: str,\n",
    "    mode: str = \"append\",\n",
    ") -> DataStreamWriter:\n",
    "    \"\"\"\n",
    "    Creates stream writer object with checkpointing at `checkpoint`\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .outputMode(mode)\n",
    "        .option(\"checkpointLocation\", checkpoint)\n",
    "        .queryName(queryName)\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining **Raw** functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def read_stream_raw(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads stream from `TWEET_SOURCE_PATH` with schema enforcement.\n",
    "    \"\"\"\n",
    "    raw_data_schema = \"date STRING, user STRING, text STRING, sentiment STRING\"\n",
    "\n",
    "    return (\n",
    "        spark.readStream.format(\"json\")\n",
    "        .schema(raw_data_schema)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .load(TWEET_SOURCE_PATH)\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def transform_raw(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms `df` to include `source_file` and `processing_time` columns.\n",
    "    \"\"\"\n",
    "    return df.select(\n",
    "        \"date\",\n",
    "        \"text\",\n",
    "        \"user\",\n",
    "        \"sentiment\",\n",
    "        input_file_name().alias(\"source_file\"),\n",
    "        current_timestamp().alias(\"processing_time\"),\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining **Bronze** functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def read_stream_bronze(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads stream from `BRONZE_DELTA`.\n",
    "    \"\"\"\n",
    "    return spark.readStream.format(\"delta\").load(BRONZE_DELTA)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def transform_bronze(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms `df` to include `timestamp`, `mention` and `cleaned_text` columns.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"timestamp\", to_timestamp(\"processing_time\"))\n",
    "        .withColumn(\"mention\", regexp_extract(col(\"text\"), \"@\\\\w+\", 0))\n",
    "        .withColumn(\"cleaned_text\", regexp_replace(col(\"text\"), \"@\\\\w+\", \"\"))\n",
    "        .select(\"timestamp\", \"mention\", \"cleaned_text\", \"sentiment\")\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining **Silver** functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def read_stream_silver(spark: SparkSession) -> DataFrame:\n",
    "    return spark.readStream.format(\"delta\").load(SILVER_DELTA)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "@pandas_udf(\"score: int, label: string\")\n",
    "def perform_model_inference(s: pd.Series) -> pd.DataFrame:\n",
    "    predictions = loaded_model.predict(s.tolist())\n",
    "    return pd.DataFrame({\n",
    "        \"score\": predictions[\"score\"].map(lambda x: int(x * 100)).tolist(),\n",
    "        \"label\": predictions[\"label\"].tolist(),\n",
    "    })\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def transform_silver(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"sentiment_analysis\", perform_model_inference(col(\"cleaned_text\")))\n",
    "        .withColumn(\"predicted_score\", col(\"sentiment_analysis.score\"))\n",
    "        .withColumn(\"predicted_sentiment\", col(\"sentiment_analysis.label\"))\n",
    "        .withColumn(\"sentiment_id\", when(col(\"sentiment\") == \"positive\", 1).otherwise(0))\n",
    "        .withColumn(\n",
    "            \"predicted_sentiment_id\",\n",
    "            when(col(\"predicted_sentiment\") == \"POS\", 1).otherwise(0),\n",
    "        )\n",
    "        .select(\n",
    "            \"timestamp\",\n",
    "            \"mention\",\n",
    "            \"cleaned_text\",\n",
    "            \"sentiment\",\n",
    "            \"predicted_score\",\n",
    "            \"predicted_sentiment\",\n",
    "            \"sentiment_id\",\n",
    "            \"predicted_sentiment_id\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining **Gold** Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def read_stream_gold(spark: SparkSession) -> DataFrame:\n",
    "    return spark.readStream.format(\"delta\").load(GOLD_DELTA)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def query_non_empty_mentions(goldDF):\n",
    "    return goldDF.filter(goldDF.mention.isNotNull()).filter(goldDF.mention != \"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def query_mention_sentiment_count(goldDF):\n",
    "    return (\n",
    "        query_non_empty_mentions(goldDF)\n",
    "        .groupby(\"mention\")\n",
    "        .agg(\n",
    "            count(when(goldDF.sentiment == \"neutral\", 1)).alias(\"neutral_count\"),\n",
    "            count(when(goldDF.sentiment == \"positive\", 1)).alias(\"positive_count\"),\n",
    "            count(when(goldDF.sentiment == \"negative\", 1)).alias(\"negative_count\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Defining ML Flow Logging Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_tp_tn_fp_fn(df):\n",
    "    true_positives = df.filter(\n",
    "        (df.sentiment_id == 1) & (df.predicted_sentiment_id == 1)\n",
    "    ).count()\n",
    "    true_negatives = df.filter(\n",
    "        (df.sentiment_id == 0) & (df.predicted_sentiment_id == 0)\n",
    "    ).count()\n",
    "    false_positives = df.filter(\n",
    "        (df.sentiment_id == 0) & (df.predicted_sentiment_id == 1)\n",
    "    ).count()\n",
    "    false_negatives = df.filter(\n",
    "        (df.sentiment_id == 1) & (df.predicted_sentiment_id == 0)\n",
    "    ).count()\n",
    "\n",
    "    return true_positives, true_negatives, false_positives, false_negatives\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_confusion_matrix(true_positives, true_negatives, false_positives, false_negatives):\n",
    "    # Create the confusion matrix\n",
    "    confusion_matrix = spark.createDataFrame(\n",
    "        [(true_positives, false_positives), (false_negatives, true_negatives)],\n",
    "        [\"Actual Positive\", \"Actual Negative\"],\n",
    "    )\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(confusion_matrix.toPandas(), annot=True, cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "\n",
    "    # Log confusion matrix image as MLflow artifact\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_precision_recall_f1(true_positives, true_negatives, false_positives, false_negatives):\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    # Store the precision, recall, and F1-score as MLflow metrics\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1_score)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_model_details():\n",
    "    client = MlflowClient()\n",
    "    details = client.get_model_version(name=MODEL_NAME, version=1)\n",
    "    mlflow.log_param(\"model_name\", details.name)\n",
    "    mlflow.log_param(\"mlflow_version\", details.version)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def log_silver_version():\n",
    "    silver_table_version = (\n",
    "        spark.sql(\"DESCRIBE HISTORY silver.delta.`{}`\".format(SILVER_DELTA))\n",
    "        .select(\"version\")\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    mlflow.log_param(\"silverDF_version\", silver_table_version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "includes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
